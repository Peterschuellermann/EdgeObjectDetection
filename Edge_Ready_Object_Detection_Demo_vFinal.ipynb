{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a96acd08",
      "metadata": {},
      "source": [
        "# Intro\n",
        "\n",
        "This notebook provides a complete \"edge-ready\" workflow for performing object detection on high-resolution satellite imagery using limited hardware resources (CPU only). It guides users through setting up the environment, downloading optical imagery of the Port of Rotterdam, and applying a lightweight YOLOv8 model to detect objects within large images using slicing techniques. Finally, the notebook demonstrates how to visualize these detections.\n",
        "\n",
        "It is designed to be a partial solution, leaving hackathon participants plenty of opportunity for scaling and optimizing.\n",
        "\n",
        "The evaluation will look at the inference time. In this notebook this is the \"Running the Model\" section. If you use preprocessing, include this in the inference block so that the execution time remains accurate. We will also evaluate the interpretation and display of results, whether in this notebook, a seperate dashboard, or a static image. Consider the evaluation a balance of speed and utility (your \"So what\" with the resulting detection).\n",
        "\n",
        "For a video walkthrough: https://drive.google.com/file/d/1LIg6HTszGtkUfZqGDlh2vc3AIAbWwN0p/view?usp=sharing "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba23fe79",
      "metadata": {},
      "source": [
        "# Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "890b5a21",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/bin/python'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "sys.executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "061c09f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install packages from requirements.txt\n",
        "!pip3 install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9ace36cd",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     sys.path.append(os.getcwd())\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Project Imports\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_logging, inspect_environment\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LOCAL_DIR\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_s3_files, download_files_parallel\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/EdgeObjectDetection/src/utils.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpsutil\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetup_logging\u001b[39m():\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "# Fix OpenCV import issues in headless environments\n",
        "# Set environment variables to prevent GUI-related errors\n",
        "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
        "\n",
        "# Make sure src is in path if needed\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.append(os.getcwd())\n",
        "\n",
        "# Project Imports\n",
        "from src.utils import setup_logging, inspect_environment\n",
        "from src.config import LOCAL_DIR\n",
        "from src.data_loader import list_s3_files, download_files_parallel\n",
        "from src.model_handler import load_model\n",
        "from src.inference import run_inference\n",
        "from src.visualization import plot_results, create_map, create_map_by_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8aae02",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adjust logging to be minimal\n",
        "setup_logging()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab4f49b",
      "metadata": {},
      "source": [
        "# Intro to the Workspace\n",
        "The goal of the challenge is to use a more constrained hardware with limited CPU cores and no GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c054411",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overview of the VM\n",
        "inspect_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a0d5758",
      "metadata": {},
      "source": [
        "# Download the Data\n",
        "We will use the Port of Rotterdam and Pan-Sharpened RGB imagery from WorldView-2 with a resolution of 0.5m. Note that this dataset will have some image gaps.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860a5ff5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Configuration & Download\n",
        "tasks = list_s3_files()\n",
        "download_files_parallel(tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34259e3b",
      "metadata": {},
      "source": [
        "# Running the Model -- The Speed Evaluation Block\n",
        "We use a pre-trained YOLOv8 Nano model to demonstrate immediate object detection on CPU for select classes. If you choose to train your own model using the labels found in spacenet/SN6_buildings/train/AOI_11_Rotterdam/summaryData/, we recommend prioritizing lightweight architectures to focus on speed. As we do not provide a GPU, we do not expect teams to train models. But we do encourage looking for other pretrained models that may be more accurate.\n",
        "\n",
        "**Note:** The inference pipeline automatically detects and crops black bars from images before processing. This preprocessing step improves detection accuracy by removing edge artifacts that can interfere with object detection. The black bar detection uses a threshold-based approach to identify and remove uniform black regions at image borders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b049ef97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup Model\n",
        "detection_model = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dddb588",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EVALUATION BLOCK.\n",
        "\n",
        "# 2. Setup Data\n",
        "folder = LOCAL_DIR\n",
        "files = sorted(glob.glob(os.path.join(folder, \"*.tif\")))[:10]\n",
        "\n",
        "# Define a simple analytics callback for demonstration\n",
        "def print_detection_count(path, result, img, transform):\n",
        "    count = len(result.object_prediction_list)\n",
        "    if count > 0:\n",
        "        # print(f\"Image {os.path.basename(path)}: Found {count} objects\")\n",
        "        pass\n",
        "\n",
        "# 3. Process Loop (Inference Only)\n",
        "geo_detections, inference_cache = run_inference(files, detection_model, analytics_callbacks=[print_detection_count])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "528ea5a9",
      "metadata": {},
      "source": [
        "# Visualize the Results\n",
        "We use a simple plot to showcase bounding boxes, labels, and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b9addc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the first 10 file paths to visualize\n",
        "# The plot_results function now supports optional parameters:\n",
        "# - max_plots: Maximum number of images to plot (default: 10)\n",
        "# - output_file: Path to save the plot (default: \"detection_plots.png\")\n",
        "plot_results(files, inference_cache, max_plots=10, output_file=\"detection_plots.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f6c6298",
      "metadata": {},
      "source": [
        "# More Advanced Visualization -- The Insights Evaluation Block\n",
        "We will demo a basic map with the original images and added bounding boxes. Your output can be in any format you like and it does not need to come from a notebook. Just be sure the evaluation team can access it.\n",
        "\n",
        "The visualization system now supports creating day-specific maps, which can help analyze temporal patterns in detections across different dates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40cefc3e",
      "metadata": {},
      "source": [
        "# Day-Specific Maps\n",
        "For datasets spanning multiple days, you can create separate maps for each day. This helps analyze temporal patterns and makes it easier to navigate large datasets. The function automatically groups images by date extracted from filenames and creates individual HTML maps for each day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f78ef8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create day-specific maps\n",
        "# This will create separate HTML files for each day found in the dataset\n",
        "# Returns a dictionary mapping day strings (YYYY-MM-DD) to file paths\n",
        "day_to_filepath = create_map_by_day(files, geo_detections)\n",
        "\n",
        "# Display the mapping\n",
        "print(\"Day-specific maps created:\")\n",
        "for day, filepath in day_to_filepath.items():\n",
        "    print(f\"  {day}: {filepath}\")\n",
        "\n",
        "# Optionally, you can open a specific day's map\n",
        "# Uncomment the lines below and replace with a specific day if you want to display it\n",
        "# from IPython.display import HTML\n",
        "# if day_to_filepath:\n",
        "#     first_day = list(day_to_filepath.keys())[0]\n",
        "#     with open(day_to_filepath[first_day], 'r') as f:\n",
        "#         HTML(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9766237",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create combined map with all detections\n",
        "# The create_map function now supports an optional output_file parameter\n",
        "m = create_map(files, geo_detections, output_file=\"map.html\")\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27456d09",
      "metadata": {},
      "source": [
        "# Helpful Hints\n",
        "To elevate your solution from a simple detection model to a comprehensive situational awareness tool, consider the following areas:\n",
        "\n",
        "**1. Preprocessing and managing compute**\n",
        "Is there a faster way to discard images unlikely to have and desired objects? What steps could help reduce the total inference needed? Any off-the-shelf masking tools available? How do you work around RAM bottlenecks?\n",
        "\n",
        "**2. Data Enrichment**\n",
        "Don't rely on imagery alone. Augment your detections with external datasets to provide context:\n",
        "OpenStreetMap (OSM): Use road networks to determine mobility corridors or building footprints to validate detection accuracy.\n",
        "Elevation Data (DEM/DSM): Incorporate terrain height to analyze vantage points or flood risks in low-lying areas like Rotterdam.\n",
        "Weather History: Does weather play a role?\n",
        "\n",
        "\n",
        "**3. Visualization Strategy**\n",
        "High-resolution satellite imagery is heavy. How will your end-user interact with it seamlessly?\n",
        "Implement Image Pyramiding and Tiling to ensure smooth zooming and panning.\n",
        "Consider converting outputs to Cloud Optimized GeoTIFFs (COGs) for efficient streaming.\n",
        "Focus on the \"Level of Detail\" show cluster markers at high altitudes and specific bounding boxes only when zoomed in.\n",
        "\n",
        "\n",
        "**4. Tactical Utility (The \"So What?\")**\n",
        "Analyze the spatial arrangement of detected objects to identify critical lines of sight, cover availability, concealment, and mobility corridors that inform both defensive positioning and offensive maneuvering. Are there military equipment specs that could be combined with the results for a more actionable plan or risk evaluation?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
